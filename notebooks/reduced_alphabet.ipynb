{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute reduced alphabet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/Documents/Projects/CI_CRUK_2025/venv/lib/python3.12/site-packages/logomaker/../tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyrepseq as prs\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = '/home/jhenderson/Documents/Projects/data_sets/tcr_sequences/CI_CRUK_datathon/'\n",
    "DATA_PATH = '/Users/james/Documents/Projects/data_sets/tcr_sequences/CI_CRUK_datathon/'\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + 'processed_data/train_fixed_lengths.csv')\n",
    "train = train.dropna(subset=['CDR3B', 'epitope', 'Assays']).reset_index(drop=True)\n",
    "train = train[train['Assays'].str.contains('mer')]\n",
    "train['epitope'] = train['epitope'].apply(lambda x: x[2:7])\n",
    "train['CDR3B'] = train['CDR3B'].apply(lambda x: x[4:14])\n",
    "aminoacids = 'ACDEFGHIKLMNPQRSTVWY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct all possible reduced epitopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['0', '1']\n",
    "possible_reduced_epitopes = [''.join(i) for i in itertools.product(letters, repeat = 5)]\n",
    "reduced_epitope_letter_counts = {epitope : [epitope.count('0'), epitope.count('1')] for epitope in possible_reduced_epitopes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_df(df, translation, columns_to_translate):\n",
    "    \n",
    "    df = df.copy()\n",
    "    translation_table = str.maketrans(translation)\n",
    "    for column in columns_to_translate:\n",
    "        df['translation_'+column] = df[column].apply(lambda x: x.translate(translation_table))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epitope_prior(translation):\n",
    "    \n",
    "    alphabet = list(translation.values()) \n",
    "    counts = {'0':alphabet.count('0'), '1':alphabet.count('1')}\n",
    "    priors = pd.Series(index=possible_reduced_epitopes)\n",
    "    for epitope in possible_reduced_epitopes:\n",
    "        priors[epitope] = ((counts['0']/20)**(reduced_epitope_letter_counts[epitope][0]))*((counts['1']/20)**(reduced_epitope_letter_counts[epitope][1]))\n",
    "         \n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_conditional(df, feature, grouping, group_weights):\n",
    "    \n",
    "    df = df.groupby(grouping).filter(lambda x: len(x) > 1)\n",
    "    conditional_pcs = df.groupby(grouping).apply(lambda x: prs.pc(x[feature]), include_groups=False)\n",
    "    adjusted_group_weights = (group_weights**2)/np.sum(group_weights**2)\n",
    "    \n",
    "    return np.sum(adjusted_group_weights * conditional_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_cross(df, feature, grouping, group_weights):\n",
    "    \n",
    "    groups = sorted(list(df.groupby(grouping)))\n",
    "    pc_group = np.sum(group_weights**2)\n",
    "    data = []\n",
    "    for ((name1, d1)), (name2, d2) in itertools.combinations(groups, 2):\n",
    "        pc_cross_group = prs.pc(d1[feature], d2[feature])\n",
    "        weight = 2*(group_weights[name1] * group_weights[name2]) / (1-pc_group)\n",
    "        data.append(weight * pc_cross_group)      \n",
    "    data = np.array(data)\n",
    "    return np.sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_background_pc(df, feature, grouping, epitope_weights):\n",
    "    \n",
    "    pc_within = pc_conditional(df, feature, grouping, epitope_weights)\n",
    "    pc_across = pc_cross(df, feature, grouping, epitope_weights)\n",
    "    pc_epitope = np.sum(epitope_weights**2)\n",
    "     \n",
    "    return pc_epitope*pc_within + (1-pc_epitope)*pc_across"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(translation, df):\n",
    "     \n",
    "    df = translate_df(df, translation, ['CDR3B', 'epitope'])\n",
    "    epitope_weights = compute_epitope_prior(translation)\n",
    "        \n",
    "    background_beta_entropy = -np.log2(estimate_background_pc(df, 'translation_CDR3B', 'translation_epitope', epitope_weights))\n",
    "    specific_beta_entropy = -np.log2(pc_conditional(df, 'translation_CDR3B', 'translation_epitope', epitope_weights))\n",
    "     \n",
    "    return background_beta_entropy - specific_beta_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation_from_list(aa_to_be_one):\n",
    "    \n",
    "    return {aa: '1' if aa in aa_to_be_one else '0' for aa in aminoacids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_algorithm(df):\n",
    "    \n",
    "    best_amino_acids_to_be_one = []\n",
    "    best_score = -np.inf\n",
    "    for i in range(20):\n",
    "        best_amino_acid_to_be_one = \"\"\n",
    "        best_local_score = -np.inf\n",
    "        for aa in aminoacids:\n",
    "            if aa not in best_amino_acids_to_be_one:\n",
    "                list_to_try = best_amino_acids_to_be_one.copy()\n",
    "                list_to_try.append(aa)\n",
    "                translation = make_translation_from_list(list_to_try)\n",
    "                score = evaluate_translation(translation, df)\n",
    "                \n",
    "                if score > best_local_score:\n",
    "                    best_amino_acid_to_be_one = aa\n",
    "                    best_local_score = score\n",
    "        \n",
    "        if best_local_score <= best_score:\n",
    "            print(\"Locally optimal set found\")\n",
    "            return best_amino_acids_to_be_one, best_score\n",
    "        \n",
    "        print(f\"Improvement found, new score: {best_local_score:.1f} bits\")\n",
    "        best_score = best_local_score\n",
    "        best_amino_acids_to_be_one.append(best_amino_acid_to_be_one)\n",
    "            \n",
    "    return best_amino_acids_to_be_one, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement found, new score: 0.1 bits\n",
      "Improvement found, new score: 0.2 bits\n",
      "Improvement found, new score: 0.4 bits\n",
      "Improvement found, new score: 0.7 bits\n",
      "Improvement found, new score: 1.6 bits\n",
      "Improvement found, new score: 2.5 bits\n",
      "Improvement found, new score: 2.9 bits\n",
      "Improvement found, new score: 3.2 bits\n",
      "Improvement found, new score: 3.4 bits\n",
      "Locally optimal set found\n"
     ]
    }
   ],
   "source": [
    "best, score = greedy_algorithm(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['R', 'E', 'Q', 'G', 'S', 'Y', 'C', 'K', 'H'],\n",
       " ['A', 'D', 'F', 'I', 'L', 'M', 'N', 'P', 'T', 'V', 'W'],\n",
       " np.float64(3.437883625079337))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best, [aa for aa in aminoacids if aa not in best], score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_translation = make_translation_from_list(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_translation, index=['Letter']).to_csv(DATA_PATH + \"/reduction_outputs/alphabet.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
